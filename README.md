## Contributors

**[Contributor Name](https://github.com/amantiwaricse))**
Contribution: Assisted in deploying the Large Language Model (LLM) locally, providing valuable guidance and technical support to successfully implement the local deployment process.

Steps to Deploy an LLM Locally

 the steps to deploy a large language model (LLM) locally with a Flask backend and a Streamlit frontend:

1. *Set Up Your Environment*:
   - Create and activate a virtual environment.
   - Install required packages using pip (e.g., Flask, Transformers, LangChain, PyPDF).

2. *Develop Your Backend*:
   - Write a Flask application that initializes and utilizes the LLM for summarization.
   - Define an endpoint (e.g., /summarize) that handles PDF file uploads and returns the summary.

3. *Develop Your Frontend*:
   - Create a Streamlit application that allows users to upload PDF files.
   - Implement functionality to send the uploaded PDF to the Flask backend and display the summary.

4. *Run and Test Locally*:
   - Start the Flask server to handle API requests.
   - Run the Streamlit application to interact with the Flask backend.

